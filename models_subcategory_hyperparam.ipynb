{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2088,"status":"ok","timestamp":1657706691668,"user":{"displayName":"altvait LTH","userId":"05295412925219522312"},"user_tz":-120},"id":"ByARPVVgPMPr"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.utils import plot_model\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2\n","import itertools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import remove_items, split_data, group_color"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_df(x):\n","    \"\"\"\n","    function to clean dataframe and keep selected columns\n","\n","    input : x, columns to keep in dataframe\n","    output: styles, modified dataframe\n","    \"\"\"\n","\n","    styles = pd.read_csv(\"data/styles.csv\", on_bad_lines='skip')\n","\n","    # drop unnecessary columns which are not needed to make recommendation\n","    styles = styles.drop([\"productDisplayName\"], axis=1)\n","    styles = styles.drop([\"year\"], axis=1)\n","    styles = styles[(styles.masterCategory=='Apparel')| (styles.masterCategory=='Footwear')]\n","    styles = styles.drop(styles[styles[\"subCategory\"]==\"Innerwear\"].index)\n","    styles = styles.dropna()\n","\n","    styles = remove_items(styles,\"subCategory\", [\"Apparel Set\", \"Dress\", \"Loungewear and Nightwear\", \"Saree\", \"Socks\"])\n","    styles[\"subCategory\"] = styles[\"subCategory\"].transform(lambda x: \"Footwear\" if(x in [\"Shoes\", \"Flip Flops\", \"Sandal\"]) else x)\n","    styles = styles.drop(labels=[6695, 16194, 32309, 36381, 40000], axis=0) # drop incomplete rows\n","    styles = styles[styles.subCategory==x]\n","\n","    # group colors to the color-wheel\n","    group_color(styles)\n","    styles.baseColour=styles.colorgroup\n","    \n","    return styles"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1090,"status":"ok","timestamp":1657706693370,"user":{"displayName":"altvait LTH","userId":"05295412925219522312"},"user_tz":-120},"id":"VVnNDifbPMPw","outputId":"bf645009-1864-45c8-d432-f7401a8478e3"},"outputs":[],"source":["topwear_df    = clean_df(\"Topwear\")\n","bottomwear_df = clean_df(\"Bottomwear\")\n","footwear_df   = clean_df(\"Footwear\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1657706693372,"user":{"displayName":"altvait LTH","userId":"05295412925219522312"},"user_tz":-120},"id":"CvMLjBFcPMPx","outputId":"e4564e62-a6c6-400f-9950-588117227d91"},"outputs":[],"source":["topwear_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1657706693372,"user":{"displayName":"altvait LTH","userId":"05295412925219522312"},"user_tz":-120},"id":"SXSHFUsaPMPx","outputId":"14b04cf1-a2ff-4bdc-b903-57b057be5dd9"},"outputs":[],"source":["bottomwear_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657706693373,"user":{"displayName":"altvait LTH","userId":"05295412925219522312"},"user_tz":-120},"id":"ICpnrJpiPMPx","outputId":"77516e72-af27-4066-8095-6e78d8076168"},"outputs":[],"source":["footwear_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encode_labels(styles):\n","    \"\"\"\n","    function to encode data \n","\n","    input : styles, dataframe to encode\n","    output: styles, (encoded) dataframe \n","            articleTypeLB, genderLB, baseColourLB, seasonLB, usageLB: all labelEncoders\n","    \"\"\"\n","\n","    articleTypeLB = LabelEncoder()\n","    genderLB      = LabelEncoder()\n","    baseColourLB  = LabelEncoder()\n","    seasonLB      = LabelEncoder()\n","    usageLB       = LabelEncoder()\n","\n","    styles['articleType'] = articleTypeLB.fit_transform(styles['articleType'])\n","    styles['gender']      = genderLB.fit_transform(styles['gender'])\n","    styles['baseColour']  = baseColourLB.fit_transform(styles['baseColour'])\n","    styles['season']      = seasonLB.fit_transform(styles['season'])\n","    styles['usage']       = usageLB.fit_transform(styles['usage'])\n","\n","    return styles, articleTypeLB, genderLB, baseColourLB, seasonLB, usageLB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# convert text fields to one-hot encoded values\n","topwear_df, top_article, top_gender, top_color, top_season, top_usage = encode_labels(topwear_df)\n","bottomwear_df, bottom_article, bottom_gender, bottom_color, bottom_season, bottom_usage = encode_labels(bottomwear_df)\n","footwear_df, foot_article, foot_gender, foot_color, foot_season, foot_usage = encode_labels(footwear_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(topwear_df['articleType'], top_article)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_array(df):\n","      \"\"\"\n","      function to fetch dataset; to give as input to model\n","      \n","      input : dataframe\n","      return: data\n","      \"\"\"\n","\n","      train_imgs = np.zeros((len(df.id), 80, 60, 3))\n","\n","      for i in range(len(df.id)):\n","          \n","        ID = df.id.iloc[i]\n","        img_path = f\"data/images/{ID}.jpg\"   \n","        img = cv2.imread(img_path)\n","      \n","        if img.shape != (80, 60, 3):\n","            img = image.load_img(img_path, target_size=(80, 60, 3))\n","        \n","        train_imgs[i] = img\n","      \n","      data = tf.data.Dataset.from_tensor_slices(\n","        (\n","          {\n","            \"images\" : train_imgs\n","          },\n","\n","          {\n","            'articleType': df[['articleType']],\n","            'gender'    : df[['gender']],\n","            'baseColour': df[['baseColour']],\n","            'season'    : df[['season']],\n","            'usage'     : df[['usage']]\n","          }\n","        )\n","      )\n","\n","      return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_layers(resnet_input, num_classes, activation, name, hparam):\n","        \"\"\"\n","        function to build model branch\n","\n","        input : resnet_input: keras.Input\n","                num_classes : number of output classes\n","                activation  : type of activation\n","                name        : output name\n","        return: final layer output\n","        \"\"\"\n","\n","        x = layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(resnet_input)\n","        x = layers.Dropout(hparam[1])(x)\n","        x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n","        x = layers.Dropout(hparam[1])(x)\n","        x = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n","        x = layers.Dropout(hparam[1])(x)\n","        x = layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n","        x = layers.Dropout(hparam[1])(x)\n","\n","        x = layers.Dense(num_classes)(x)\n","        x = layers.Activation(activation, name=name)(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_model(width, height, list_branches, hparam):\n","  \"\"\"\n","  function to build model for categories\n","\n","  input : width, image width \n","          height, image height\n","  return: keras.Model\n","  \"\"\"\n","\n","  resnet50 = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(80, 60, 3))\n","  resnet50.trainable=False\n","\n","  inputs = keras.Input(shape=(width, height, 3), name=\"images\")\n","  x = layers.RandomFlip(\"horizontal\")(inputs)\n","  x = layers.RandomRotation(factor=0.2)(x)\n","  x = layers.RandomZoom(height_factor=0.1, width_factor=0.1)(x)\n","  x = layers.RandomContrast(factor=0.2)(x)\n","\n","  x = resnet50(x, training=False)\n","  x = layers.Flatten()(x)\n","  x = layers.Dense(1024, activation='relu')(x)\n","  \n","  article_branch = add_layers(x, len(list_branches[0].classes_), 'softmax', 'articleType', hparam)\n","  gender_branch  = add_layers(x, len(list_branches[1].classes_), 'softmax', 'gender', hparam)\n","  color_branch   = add_layers(x, len(list_branches[2].classes_), 'softmax', 'baseColour', hparam)\n","  season_branch  = add_layers(x, len(list_branches[3].classes_), 'softmax', 'season', hparam)\n","  usage_branch   = add_layers(x, len(list_branches[4].classes_), 'softmax', 'usage', hparam)\n","\n","  model = keras.Model(inputs=inputs,\n","                      outputs=[article_branch, gender_branch, color_branch, season_branch, usage_branch])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_branches = [[top_article, top_gender, top_color, top_season, top_usage],\n","                     [bottom_article, bottom_gender, bottom_color, bottom_season, bottom_usage], \n","                     [foot_article, foot_gender, foot_color, foot_season, foot_usage]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs_list = [5, 10, 15]\n","dropout_list = [0.1, 0.25]\n","learning_rates = [1e-3, 1e-4]\n","regularization_list = [0.1, 0.25]\n","\n","def get_hyperparameter_combinations(epochs, dropout, learning_rates, reg):\n","  \"\"\"\n","  function to get all combinations of hyperparameters\n","\n","  input : list of different hyperparameters\n","  return: all combinations of hyperparameters\n","  \"\"\"\n","  hyperparameters_list = [epochs, dropout, learning_rates, reg]\n","  hyperparameters_comb = list(itertools.product(*hyperparameters_list))\n","  return hyperparameters_comb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def hyperparameter_tuning(hyperparameters_comb, list_branch, sub_train, sub_val, sub_test):\n","        \"\"\"\n","        function to get get the best hyperparameters for the model\n","\n","        input : hyperparameters combinations, model type(top, bottom, foot), and corresponding dataset\n","        return: best hyperparameters for the model\n","        \"\"\"\n","        best_loss = float('inf')\n","        for i, hparam in enumerate(hyperparameters_comb):\n","                print('\\nCOMBO', i, hparam)\n","                test_net = build_model(80, 60, list_branches=list_branch, hparam=hparam)\n","                test_net.compile(optimizer='adam',\n","                                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","                                metrics=['accuracy'])\n","                with tf.device(\"/gpu:0\"):\n","                        test_stats = test_net.fit(sub_train, epochs=hparam[0], validation_data=sub_val)\n","\n","                score = test_net.evaluate(sub_test)\n","                current_loss = score[0]\n","                print('Current loss for', i, 'th combination is:', current_loss)\n","\n","                if current_loss < best_loss:\n","                        best_loss = current_loss\n","                        best_idx = i\n","                        best_net = test_net\n","                        best_stats = test_stats\n","        return best_idx, best_stats, best_net, best_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model for topwear\n","batch_size = 32\n","top_train, top_val, top_test = split_data(create_array(topwear_df), batch_size)\n","hyperparameters_comb = get_hyperparameter_combinations(epochs_list, dropout_list, learning_rates, regularization_list)\n","best_idx, best_stats, best_net, best_acc = hyperparameter_tuning(hyperparameters_comb, list_branch=list_branches[0], sub_train=top_train, sub_val=top_val, sub_test=top_test)\n","print('Best validation score: ', best_acc)\n","print('Dropout strength: ', hyperparameters_comb[best_idx][1])\n","print('Number of epochs: ', hyperparameters_comb[best_idx][0])\n","print('Learning rate used: ', hyperparameters_comb[best_idx][2])\n","print('Regularization strength: ', hyperparameters_comb[best_idx][3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model for bottomwear\n","batch_size = 32\n","bottom_train, bottom_val, bottom_test = split_data(create_array(bottomwear_df),batch_size)\n","hyperparameters_comb = get_hyperparameter_combinations(epochs_list, dropout_list, learning_rates, regularization_list)\n","best_idx, best_stats, best_net, best_acc = hyperparameter_tuning(hyperparameters_comb, list_branch=list_branches[1], sub_train=bottom_train, sub_val=bottom_val, sub_test=bottom_test)\n","print('Best validation score: ', best_acc)\n","print('Dropout strength: ', hyperparameters_comb[best_idx][1])\n","print('Number of epochs: ', hyperparameters_comb[best_idx][0])\n","print('Learning rate used: ', hyperparameters_comb[best_idx][2])\n","print('Regularization strength: ', hyperparameters_comb[best_idx][3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model for footwear\n","batch_size = 32\n","foot_train, foot_val, foot_test = split_data(create_array(footwear_df), batch_size)\n","hyperparameters_comb = get_hyperparameter_combinations(epochs_list, dropout_list, learning_rates, regularization_list)\n","best_idx, best_stats, best_net, best_acc = hyperparameter_tuning(hyperparameters_comb, list_branch=list_branches[2], sub_train=foot_train, sub_val=foot_val, sub_test=foot_test)\n","print('Best validation score: ', best_acc)\n","print('Dropout strength: ', hyperparameters_comb[best_idx][1])\n","print('Number of epochs: ', hyperparameters_comb[best_idx][0])\n","print('Learning rate used: ', hyperparameters_comb[best_idx][2])\n","print('Regularization strength: ', hyperparameters_comb[best_idx][3])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"model2.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
