{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_items, split_data, group_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(csv_path=\"data/styles.csv\"):\n",
    "      \"\"\"\n",
    "      function to fetch and clean the data\n",
    "\n",
    "      input : csv_path, path to .csv file\n",
    "      return: styles, dataframe\n",
    "      \"\"\"\n",
    "\n",
    "      styles = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "\n",
    "      # drop unnecessary columns which are not needed to make recommendation\n",
    "      styles = styles.drop([\"productDisplayName\"], axis=1) \n",
    "      styles = styles.drop([\"year\"], axis=1) \n",
    "      styles = styles[(styles.masterCategory=='Apparel')|(styles.masterCategory=='Footwear')]\n",
    "      styles = styles.drop(styles[styles[\"subCategory\"]==\"Innerwear\"].index)\n",
    "      styles = styles.dropna()\n",
    "\n",
    "      styles = remove_items(styles,\"subCategory\", [\"Apparel Set\", \"Dress\", \"Loungewear and Nightwear\", \"Saree\", \"Socks\"])\n",
    "      styles[\"subCategory\"] = styles[\"subCategory\"].transform(lambda x: \"Footwear\" if (x in [\"Shoes\", \"Flip Flops\", \"Sandal\"]) else x)\n",
    "      styles = styles.drop(labels = [6695, 16194, 32309, 36381, 40000], axis=0) # drop incomplete rows\n",
    "\n",
    "      # group colors to the color-wheel\n",
    "      group_color(styles) \n",
    "\n",
    "      return styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = clean_df()\n",
    "styles[\"subCategory\"].unique() # sanity check after cleaning df: three subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "styles[\"subCategory\"] = le.fit_transform(styles[\"subCategory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(df):\n",
    "      \"\"\"\n",
    "      function to fetch dataset \n",
    "\n",
    "      input : dataframe\n",
    "      return: dataset\n",
    "      \"\"\"\n",
    "\n",
    "      train_imgs = np.zeros((len(df.id), 80, 60, 3))\n",
    "\n",
    "      for i in range(len(df.id)):      \n",
    "        ID = df.id.iloc[i]\n",
    "        img_path = f\"data/images/{ID}.jpg\"   \n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img.shape != (80, 60, 3):\n",
    "          img = image.load_img(img_path, target_size=(80, 60, 3))\n",
    "\n",
    "        train_imgs[i] = img\n",
    "\n",
    "      data = tf.data.Dataset.from_tensor_slices(\n",
    "      (\n",
    "        {\n",
    "          \"images\": train_imgs\n",
    "        },\n",
    "        \n",
    "        {\n",
    "          \"subCategory\": df[[\"subCategory\"]]\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "\n",
    "      return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(width, height, num_classes, hparam):\n",
    "        \"\"\"\n",
    "        function to build model for subCategories\n",
    "\n",
    "        input : width, image width \n",
    "                height, image height\n",
    "                num_classes, number of classes\n",
    "        return: keras.Model\n",
    "        \"\"\"\n",
    "\n",
    "        resnet50 = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(80, 60, 3))\n",
    "        resnet50.trainable=False\n",
    "\n",
    "        inputs = keras.Input(shape=(width,height,3), name=\"images\")\n",
    "\n",
    "        x = layers.RandomFlip(\"horizontal\")(inputs)\n",
    "        x = layers.RandomRotation(factor=0.2)(x)\n",
    "        x = layers.RandomZoom(height_factor=0.1, width_factor=0.1)(x)\n",
    "        x = layers.RandomContrast(factor=0.2)(x)\n",
    "\n",
    "        x = resnet50(x, training=False)\n",
    "        x = layers.Conv2D(32, (2, 2), activation='relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(1024, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n",
    "        x = layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n",
    "        x = layers.Dropout(hparam[1])(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n",
    "        x = layers.Dropout(hparam[1])(x)\n",
    "        x = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n",
    "        x = layers.Dropout(hparam[1])(x)\n",
    "        x = layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l=hparam[3]))(x)\n",
    "        x = layers.Dropout(hparam[1])(x)\n",
    "        x = layers.Dense(len(num_classes))(x)\n",
    "        x = layers.Activation('softmax', name='subCategory')(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sub_train, sub_val, sub_test = split_data(create_array(styles), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [5, 10, 15]\n",
    "dropout_list = [0.1, 0.25]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "regularization_list = [0.1, 0.25]\n",
    "\n",
    "def get_hyperparameter_combinations(epochs, dropout, learning_rates, reg):\n",
    "  \"\"\"\n",
    "  function to get all combinations of hyperparameters\n",
    "\n",
    "  input : list of different hyperparameters\n",
    "  return: all combinations of hyperparameters\n",
    "  \"\"\"\n",
    "  hyperparameters_list = [epochs, dropout, learning_rates, reg]\n",
    "  hyperparameters_comb = list(itertools.product(*hyperparameters_list))\n",
    "  return hyperparameters_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(hyperparameters_comb):\n",
    "        \"\"\"\n",
    "        function to get get the best hyperparameters for the model\n",
    "\n",
    "        input : hyperparameters combinations\n",
    "        return: best hyperparameters for the model\n",
    "        \"\"\"\n",
    "        best_acc = 0\n",
    "        for i, hparam in enumerate(hyperparameters_comb):\n",
    "                print('\\nCOMBO', i, hparam)\n",
    "                test_net = build_model(80, 60, num_classes=le.classes_, hparam=hparam)\n",
    "                test_net.compile(optimizer='adam',\n",
    "                                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                metrics=['accuracy'])\n",
    "                with tf.device(\"/gpu:0\"):\n",
    "                        test_stats = test_net.fit(sub_train, epochs=hparam[0], validation_data=sub_val)\n",
    "\n",
    "                score = test_net.evaluate(sub_test)\n",
    "                current_acc = score[1]\n",
    "                print('Current accuracy for', i, 'th combination is:', current_acc)\n",
    "\n",
    "                if current_acc > best_acc:\n",
    "                        best_acc = current_acc\n",
    "                        best_idx = i\n",
    "                        best_net = test_net\n",
    "                        best_stats = test_stats\n",
    "        return best_idx, best_stats, best_net, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_comb = get_hyperparameter_combinations(epochs_list, dropout_list, learning_rates, regularization_list)\n",
    "best_idx, best_stats, best_net, best_acc = hyperparameter_tuning(hyperparameters_comb)\n",
    "print('Best Test score : ', best_acc)\n",
    "print('Optimal epochs : ', hyperparameters_comb[best_idx][1])\n",
    "print('Optimal dropout : ', hyperparameters_comb[best_idx][0])\n",
    "print('Learning rate used : ', hyperparameters_comb[best_idx][2])\n",
    "print('Regularization strength : ', hyperparameters_comb[best_idx][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net.evaluate(sub_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
